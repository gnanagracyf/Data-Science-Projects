{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Using Python Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi problem - transport people in a parking lot to four different locations (R,G,Y,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement is a class of machine learning where an agent learns how to behave in the environment by performing actions\n",
    "and thereby drawing intuitions and seeing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few applications that motivate you to build reinforcement systems,\n",
    "\n",
    "Self Driving Cars, \n",
    "Gaming, \n",
    "Robotics,\n",
    "Recommendation Systems,\n",
    "Advertising and Marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning supports automation by learning from the environment it is present in, so does Machine Learning and \n",
    "Deep Learning, not the same strategy, but both support automation. So, why Reinforcement Learning?\n",
    "\n",
    "It’s very much like the natural learning process wherein, the process/the model would be receiving feedback as to whether it \n",
    "has performed well or not. Deep Learning and Machine Learning, are learning processes as well, but which are most focussed on\n",
    "finding patterns in the existing data. Reinforcement Learning, on the other hand, does this learning by trial and error method, \n",
    "and eventually, gets to the right actions or the global optimum. The significant additional advantage of Reinforcement Learning\n",
    "is that we need not provide the whole training data as in Supervised Learning. Instead, a few chunks would suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem Statement:\n",
    "“There are 4 locations (labelled by different letters), and our job is to pick up the passenger at one location and drop\n",
    "him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is\n",
    "also a 10 point penalty for illegal pick-up and drop-off actions.” (Source: https://gym.openai.com/envs/Taxi-v2/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env is the core of OpenAi Gym, which is the unified environment interface. The following are the env methods that would be quite helpful to us:\n",
    "\n",
    "env.reset: Resets the environment and returns a random initial state.\n",
    "env.step(action): Step the environment by one timestep.\n",
    "\n",
    "env.step(action) returns the following variables\n",
    "\n",
    "observation: Observations of the environment.\n",
    "reward: If your action was beneficial or not\n",
    "done: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "info: Additional info such as performance and latency for debugging purposes\n",
    "env.render: Renders one frame of the environment (helpful in visualizing the environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part \n",
    "of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
    "\n",
    "In the environment, there are four possible locations where you can drop the passengers in the taxi which are: R, G, Y, B or\n",
    "    [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates if you can interpret the above-rendered environment as a coordinate \n",
    "    axis.\n",
    "\n",
    "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger\n",
    "locations and destination locations to come to a total number of states for our taxi environment; there are four (4) destination\n",
    "s and five (4 + 1) passenger locations. So, our taxi environment has 5×5×5×4=500 total possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we have six possible actions: pickup, drop, north, east, south, west"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " action space: the set of all the actions that our agent can take in a given state.\n",
    "        \n",
    "That the taxi cannot perform certain actions in certain states due to walls. In the environment’s code,\n",
    "we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Taxi environment is created, there is an initial Reward table that’s also created, called P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The reward table\n",
    "env.P[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 512\n",
      "Penalties incurred: 177\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: {i}\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "#Q Learning\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "# Init arbitary values\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    # Init Vars\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Check the action space\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Check the learned values\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Update the new value\n",
    "        new_value = (1 - alpha) * old_value + alpha * \\\n",
    "            (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.41484698,  -2.27325184,  -2.39970199,  -2.36120876,\n",
       "       -10.80978258, -10.59069293])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.72\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
