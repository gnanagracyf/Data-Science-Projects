{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a subset of AI which provides machines the ability to learn automatically and improve from experience without being explicitly programmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Types of ML : Supervised learning, Unsupervised Learning, Reinforcement Learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a type of ML where an agent learns to behave in an environment by performing actions and seeing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "Agent : RL algorithm that learns from trial and error.\n",
    "Environment : The world through which the agent moves.\n",
    "Action : All possible steps an agent can take.\n",
    "State : Current condition returned by the environment.\n",
    "Reward : An instant return from environment to appraise the last action.\n",
    "Policy : The approach that agent uses to determine the next action based on current state.\n",
    "Value : The expected long-term return with discount,  as opposed to short-term reward R.\n",
    "Action-Value : This is similar to value, except it takes an extra parameter, the current action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration : exploring and capturing more information about environment\n",
    "Exploitation : Using already known exploited information to heighten the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math behind RL is Markov's Decision Process : Find shortest path with maximum possible cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q- Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#There are 4 rooms and the agent explores all the ways to find way out ( room 5) with maximum reward.\n",
    " Room 0 is connected to 4\n",
    " Room 1 is connected to 3 & 5\n",
    " Room 2 is connected to 3\n",
    " Room 3 is connected to 4 & 1\n",
    " Room 4 is connected to 3 & 5\n",
    " Room 5 is connected to itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2 -> \n",
    "  <- 3   ->\n",
    "     ||  <-  1\n",
    "     ||     ||\n",
    "     ||     ||\n",
    "0 -> ||     ||\n",
    "  <- 4 <-   ||\n",
    "       ->   5\n",
    "\n",
    "\n",
    "Rewards are 100 points to go directly to room 5 ..1-> 5 & 4->5 ..\n",
    " If path exists then reward 0 eg : 2->3 ->1 -> 5, so  2->3 is reward 0 coz path exists and also 3->1 reward zero\n",
    " For rest of the non-existing path the reward is -1 ( null )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reward matrix\n",
    "\n",
    "    \n",
    "    #\n",
    "#                Action\n",
    "#       state  0   1   2   3  4   5\n",
    "#    R = [0  -1  -1  -1  -1  0  -1\n",
    "#         1               0      100\n",
    "#         2               0\n",
    "#         3       0   0      0\n",
    "#         4  0            0      100\n",
    "#         5       0          0   100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Construct Q Matrix\n",
    "Q(state,Action ) = R(state,action) + Gamma * Max( Q(next_State,all actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma towards zero - then exploitation \n",
    "Gamma towards 1 - then exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q - Learning Algorithm\n",
    "\n",
    "1. Set gamma param and env rewards in matrix R\n",
    "2. Intialize Q matrix to 0\n",
    "3. Select random initial state\n",
    "4. Set initial_State = current_state\n",
    "5. Select one among all possible actions for current selected state\n",
    "6. using this selection, consider going to next state\n",
    "7. Get max Q value based on all possible actions\n",
    "8. Compute Q value with above formula\n",
    "9. Repeat the above steps untill current_State = goal_State ( which is 5 here )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# R matrix\n",
    "\n",
    "R = np.matrix([[ -1, -1, -1, -1, 0, -1],\n",
    "               [ -1, -1, -1, 0, -1, 100],\n",
    "               [ -1, -1, -1, 0, -1, -1],\n",
    "               [ -1, 0 , 0 , -1, 0, -1],\n",
    "               [ -1, 0, 0, -1, -1, 100],\n",
    "               [ -1, 0, -1, -1, 0, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q matirx\n",
    "Q = np.matrix(np.zeros([6,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma learning rate\n",
    "gamma = 0.8 # Change it further and check in tests runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state  ( usually chosen ar random)\n",
    "initial_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return all available actions in the state given as argument. From R matrix choose values >0 ...possible moves from \n",
    "#the given state\n",
    "def available_actions(state):\n",
    "    current_state_row = R[state]\n",
    "    av_act = np.where(current_state_row >= 0 ) [1]\n",
    "    return av_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available action in the choosen state\n",
    "available_act = available_actions(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose which actions to be performed within the range of all available actions\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act,1))\n",
    "    return next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next action to be performed\n",
    "action = sample_next_action(available_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update Q matrix according to path selected and then Q learning algorithm\n",
    "def update(current_state, action, gamma):\n",
    "    max_index  = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    #Q- Learning Formula\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(initial_state, action, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training over 10000 iterations \n",
    "for i in range(10000):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    actions = sample_next_action(available_act)\n",
    "    update(current_state,actions,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q MAtrix\n",
      "[[  0.    0.    0.    0.   80.    0. ]\n",
      " [  0.    0.    0.   64.    0.  100. ]\n",
      " [  0.    0.    0.   64.    0.    0. ]\n",
      " [  0.   80.   51.2   0.   80.    0. ]\n",
      " [  0.   80.   51.2   0.    0.  100. ]\n",
      " [  0.   80.    0.    0.   80.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the trained Q matrix\n",
    "print (\"Trained Q MAtrix\")\n",
    "print (Q/np.max(Q) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected path\n",
      "[4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "goal_state = 5\n",
    "#Best sqg path starting from 2 -> 2, 3, 1, 5\n",
    "\n",
    "current_state = 4\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 5:\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    \n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size=1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "        \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "    \n",
    "print(\"selected path\")\n",
    "print(steps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
