{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.',\n",
       " 'My mother spends a lot of time driving my brother around to baseball practice.',\n",
       " 'Some health experts suggest that driving may cause increased tension and blood pressure.',\n",
       " 'I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.',\n",
       " 'Health professionals say that brocolli is good for your health.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=56)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# stem token\n",
    "texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.081*\"brocolli\" + 0.081*\"good\"'), (1, '0.072*\"drive\" + 0.043*\"pressur\"')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x23a0b024d68>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.5), (1, 0.5)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[dictionary.doc2bow(doc_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x23a0af4fe80>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"')\n",
      "(1, '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.show_topics()\n",
    "for topic in topics:\n",
    "    print (topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.74783873126217204), (1, 0.25216126873782796)]\n",
      "[(0, 0.098311991610900665), (1, 0.0030628003386857896)]\n"
     ]
    }
   ],
   "source": [
    "# get_document_topics for a document with a single token 'user'\n",
    "text = [\"brocolli\"]\n",
    "bow = dictionary.doc2bow(text)\n",
    "print ( ldamodel.get_document_topics(bow))\n",
    "### get_document_topics [(0, 0.74568415806946331), (1, 0.25431584193053675)]\n",
    "\n",
    "# get_term_topics for the token user\n",
    "print ( ldamodel.get_term_topics(\"brocolli\", minimum_probability=0.000001))\n",
    "### get_term_topics:  [(0, 0.1124525558321441), (1, 0.006876306738765027)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.94671405083268068), (1, 0.053285949167319269)]\n",
      "[(0, 0.06012114415014623), (1, 0.93987885584985376)]\n",
      "[(0, 0.051489640858633966), (1, 0.94851035914136606)]\n",
      "[(0, 0.95234283258192631), (1, 0.047657167418073701)]\n",
      "[(0, 0.92130572388614085), (1, 0.078694276113859152)]\n"
     ]
    }
   ],
   "source": [
    "#The document clearly states that it returns \n",
    "#topic distribution for the given document bow, as a list of (topic_id, topic_probability) 2-tuples.\n",
    "prob = ldamodel.get_document_topics(corpus)\n",
    "for p in prob:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_prob_tuple = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0,\n",
       "   '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"'),\n",
       "  [(0, 0.94671431221401936), (1, 0.05328568778598073)]),\n",
       " ((0,\n",
       "   '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"'),\n",
       "  [(0, 0.060128471572510375), (1, 0.93987152842748967)]),\n",
       " ((0,\n",
       "   '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"'),\n",
       "  [(0, 0.051467187193641684), (1, 0.94853281280635826)]),\n",
       " ((0,\n",
       "   '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"'),\n",
       "  [(0, 0.95234137640183314), (1, 0.047658623598166763)]),\n",
       " ((0,\n",
       "   '0.081*\"brocolli\" + 0.081*\"good\" + 0.059*\"mother\" + 0.059*\"brother\" + 0.058*\"eat\" + 0.058*\"health\" + 0.035*\"like\" + 0.035*\"well\" + 0.035*\"feel\" + 0.035*\"never\"'),\n",
       "  [(0, 0.92128252511230679), (1, 0.078717474887693206)]),\n",
       " ((1,\n",
       "   '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"'),\n",
       "  [(0, 0.94671477269895621), (1, 0.053285227301043729)]),\n",
       " ((1,\n",
       "   '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"'),\n",
       "  [(0, 0.060108522809792522), (1, 0.93989147719020738)]),\n",
       " ((1,\n",
       "   '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"'),\n",
       "  [(0, 0.051468599567922692), (1, 0.94853140043207729)]),\n",
       " ((1,\n",
       "   '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"'),\n",
       "  [(0, 0.95234111838964342), (1, 0.047658881610356633)]),\n",
       " ((1,\n",
       "   '0.072*\"drive\" + 0.043*\"pressur\" + 0.043*\"health\" + 0.043*\"suggest\" + 0.043*\"increas\" + 0.043*\"caus\" + 0.043*\"expert\" + 0.043*\"may\" + 0.043*\"blood\" + 0.043*\"tension\"'),\n",
       "  [(0, 0.92128462183053406), (1, 0.078715378169465874)])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_prob_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
